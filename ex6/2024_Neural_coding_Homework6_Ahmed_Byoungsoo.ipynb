{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oxLV72clE080"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Math, Latex\n",
        "\n",
        "# Libraries you might need\n",
        "import matplotlib as mpl\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import sklearn.metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H57PQa0nFepR"
      },
      "source": [
        "# Homework 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQsAp_I-FFMR"
      },
      "source": [
        "## Exercise 1\n",
        "\n",
        "Let random variable $X$ be a discrete random variable with values\n",
        "{−2, −1, 0, 1, 2}, obtained with the same probability $p = \\tfrac15$. Let\n",
        "the second random variable $Y$ be $Y = X^2$.\n",
        "\n",
        "(a) Compute analytically $cov(X, Y)$ (2 points)\n",
        "\n",
        "(b) Compute the mutual information between X and Y in bits (2 points).\n",
        "\n",
        "(c) Simulate 100, 1000, 10000 data realization of these processes and compute the covariances and mutual information based on the data. (2 points)\n",
        "\n",
        "*+1 bonus point if you compute the mutual information both with a library and yourself with using the formula.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEiVZip4IJV8"
      },
      "source": [
        "**Answer (a)**\n",
        "\n",
        "\n",
        "$cov(X,Y)= E[XY]-E[X]E[Y]$\n",
        "$E[X]=0$\n",
        "$E[Y]=\\frac{10}{5} =2$\n",
        "\n",
        "$E[XY]=0$\n",
        "\n",
        "\n",
        "Thus, $cov(X,Y)=0-0*2=0$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ne039ma6IpBc"
      },
      "source": [
        "\n",
        "![Alt text](image.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer (b)**\n",
        "$X=\\left\\{ -2,-1,0,1,2\\right\\} ,Y=\\left\\{ 4,1,0,1,4\\right\\} \\\\\n",
        "P\\left( X=-2,Y=4\\right) =\\dfrac{1}{5},P\\left( X=1,Y=1\\right) =\\dfrac{1}{5}\\\\\n",
        "P\\left( X=-1,Y=1\\right) =\\dfrac{1}{5},P\\left( X=2,Y=4\\right) =\\dfrac{1}{5}\\\\\n",
        "P\\left( X=0,Y=0\\right) =\\dfrac{1}{5},\\\\\n",
        "P\\left( Y=4\\right) =\\dfrac{2}{5},P\\left( Y=1\\right) =\\dfrac{2}{5},P\\left( Y=0\\right) =\\dfrac{1}{5}$\n",
        "\n",
        "\n",
        "$\\\\\n",
        "I\\left( X;Y\\right) =\\sum _{x,y}P\\left( X=x,Y=y\\right) \\log \\dfrac{P\\left( x=x,Y=y\\right) }{P\\left( x=x\\right) P\\left( Y=y\\right) }$\n",
        "\n",
        "\n",
        "$=P\\left( X=-2,Y=4\\right) \\log \\dfrac{P\\left( X=-2,Y=4\\right) }{P\\left( X=-2\\right) P\\left( Y=4\\right) }\n",
        "+.\\ldots \\\\\n",
        "=\\dfrac{4}{5}\\log \\left( \\dfrac{5}{2}\\right) +\\dfrac{1}{5}\\log \\left( 5\\right) $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.5219280948873626\n",
            "1.5219280948873624\n"
          ]
        }
      ],
      "source": [
        "# Bonus\n",
        "X=np.array([-2,-1,0,1,2])\n",
        "Y=X**2\n",
        "mut_inf=sklearn.metrics.mutual_info_score(X,Y)/np.log(2)\n",
        "print(mut_inf)\n",
        " \n",
        "print(4/5*np.log2(5/2)+1/5*np.log2(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "N=100\n",
            " Cov= 0.1566666666666665\n",
            "Mutual Information fomrula: 1.4590763663396218\n",
            "Mutual Information sklearn: 1.4590763663396225\n",
            "N=100\n",
            " Cov= 0.020108108108108213\n",
            "Mutual Information fomrula: 1.5338472157802887\n",
            "Mutual Information sklearn: 1.533847215780288\n",
            "N=100\n",
            " Cov= -0.01681968196819662\n",
            "Mutual Information fomrula: 1.5246349601791245\n",
            "Mutual Information sklearn: 1.524634960179126\n"
          ]
        }
      ],
      "source": [
        "def joint_probability(x, y):\n",
        "    n = len(x)\n",
        "    xy_pairs = list(zip(x, y))\n",
        "    unique_pairs, counts = np.unique(xy_pairs, axis=0, return_counts=True)\n",
        "    return {tuple(pair): count/n for pair, count in zip(unique_pairs, counts)}\n",
        "def mutual_information(P_XY, P_X, P_Y):\n",
        "    mi = 0\n",
        "    for xy in P_XY:\n",
        "        x, y = xy\n",
        "        if P_XY[xy] > 0:\n",
        "            mi += P_XY[xy] * np.log2(P_XY[xy] / (P_X[x] * P_Y[y]))\n",
        "    return mi\n",
        "def generate_data(N=100, seed=12):\n",
        "\n",
        "    rng= np.random.RandomState(seed)\n",
        "    X=rng.choice([-2,-1,0,1,2],size=N)\n",
        "    Y=X**2\n",
        "\n",
        "    cov_100=np.cov(X,Y)\n",
        "    print(f'N=100\\n Cov= {cov_100[0,1]}')\n",
        "\n",
        "    P_XY = joint_probability(X, Y)\n",
        "\n",
        "    # Calculate marginal probability distributions\n",
        "    P_X = {x: sum(P_XY[pair] for pair in P_XY if pair[0] == x) for x in set(X)}\n",
        "    P_Y = {y: sum(P_XY[pair] for pair in P_XY if pair[1] == y) for y in set(Y)}\n",
        "\n",
        "\n",
        "    MI_formula = mutual_information(P_XY, P_X, P_Y)\n",
        "\n",
        "    print(f\"Mutual Information fomrula: {MI_formula}\")\n",
        "    \n",
        "\n",
        "    MI_sklearn = sklearn.metrics.mutual_info_score(X, Y)/np.log(2)\n",
        "\n",
        "    print(f\"Mutual Information sklearn: {MI_sklearn}\")\n",
        "\n",
        "generate_data(N=100, seed=1)\n",
        "generate_data(N=1000, seed=1)\n",
        "generate_data(N=10000, seed=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccwv-Yx2HBbn"
      },
      "source": [
        "\n",
        "---\n",
        "## Exercise 2\n",
        "Suppose that we have a neuron which, in a given time period, will fire\n",
        "with probability 0.2, yielding a Bernoulli distribution for the neuron’s firing (denoted by the random variable R = 0 or 1) with p(R = 1) = 0.2.\n",
        "\n",
        "(a) Compute the entropy H(R) of this distribution (calculated in bits, i.e., using the base 2 logarithm)? (1 point)\n",
        "\n",
        "(b) Now lets add a stimulus to the picture. Suppose that we think this\n",
        "neuron's activity is related to a light flashing in the eye. Let us say that\n",
        "the light is flashing in a given time period with probability 0.2. Call this\n",
        "stimulus random variable S. If there is a flash, the neuron will fire with\n",
        "probability 1/2. If there is no flash, the neuron will fire with probability\n",
        "1/8. Call the random variable describing whether the neuron fires or not\n",
        "R. Compute the mutual information I(S : R)? (2 points)\\\n",
        "*Hint: First, confirm that H(R) is the same as above by computing p(R).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG7XOHyVJ9A0"
      },
      "source": [
        "**Answer (a)**\n",
        "\n",
        "$H=-\\sum P\\left( R\\right) \\log_2 P\\left( R\\right) \\\\\n",
        "=-\\left[ 0.2\\log_2 \\left( 0.2\\right) +0.8\\log_2 \\left( 0.8\\right) \\right] \\\\\n",
        "=0.7219$ bits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIwPtjX2KJUI"
      },
      "source": [
        "**Answer (b)**\n",
        "\n",
        "$P\\left( S=1\\right) =0.2,P\\left( S=0\\right) =0.8\\\\\n",
        "P\\left(  R=1| S=1\\right) =\\dfrac{1}{2},P\\left(  R=1| S=0\\right) =\\dfrac{1}{8}\\\\\n",
        "$\n",
        "\n",
        "$\n",
        "P\\left(  R=0| S=1\\right) =\\dfrac{1}{2},P\\left( R=0|S=0\\right) =\\dfrac{7}{8}\\\\\n",
        "P\\left( R,S\\right) =P\\left(  R\\left| S\\right) P( S\\right) \\\\\n",
        "\\\\\n",
        " P\\left( R=1\\right) =\\sum _{s}P\\left( R=1\\right| S=s) P\\left( S=s\\right) \\\\\n",
        "=  \\dfrac{1}{2} \\left( 0.2\\right) +\\left( \\dfrac{1}{8}\\right) \\left( 0.8\\right) \\\\\n",
        "=0.2\\\\\n",
        "   P\\left( R=0\\right) =\\left( \\dfrac{1}{2}\\right) ( 0.2) + \\dfrac{7}{8}(0.8) =0.8$ \n",
        "\n",
        "Thus $H(R)$ is the same. \n",
        "\n",
        "$H(R|S) = - \\sum_S P(S=s) \\sum_R P(R=r|S=s) \\log_2 (P(R=r|S=s))\n",
        "         = 0.634$\n",
        "\n",
        "         \n",
        "$I(R:S)= H(R) - H(R|S) = 0.7219 - 0.634 = 0.087$ bits"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
